Pass a pointer to the AGENT_DATA and PARAMS to the routines that run on host and device.

Only keep track of a subset of the theta values with lambda values.

Write position, angle, and action to a file for playback.


Change test to do restarts?

Use bit-wise operations instead of % and mod when dividing or taking modulo by a number which is power of 2.

Modify the test to measure average time to failure for the specified number of attempts, subject to a maximum number of time steps

-------
Sharing
-------
Average the theta values across all agents
(done)	Straight average, all agents given the same weight
			Exclude values that have never been updated
			Give extra weight to agents that 'own' a section of the state space.
		Keep track of the 'update weight' which is the sum of the lambda amounts used to update each theta, and when sharing use a weighted average based on the 'update weights' 
		Use update weights with initial 0's for all theta's all agents.  Then, the only values theta values will be from failures.
	
record all state, action, result, state action values for the episode, then have all agents update their parameters by using the same

Have short restart periods that are handled within the learning kernel.  Force a randomization of the state similar to after a failure for all states within.

---------------
Differentiation
---------------
Partition the state space - each agent starts in their sector of state space whenever their state is randomized.

Use different re-start intervals

Assign a wider range for initial states


------------------------------
Optimization
------------------------------
Use a circular buffer for eligibity traces and feature number --- This would increase non-coalesced global memory accesses, many not work well.
Cache the last feature and <other values>  check the cache first before going to global memory for the values.

Try using epsilon - allow threads to learn more
